{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline playground for 12+ classifiers (0.11860 LB)\n",
    "I want to **provide a clean, simple and beginner friendly template** that makes use of a flexible [scikit-learn pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). I aim to **automate the datacleaning and feature engineering** as much as possible while **allowing for fast iteration**. \n",
    "\n",
    "The **total training and processing time of the pipeline for 12 classifiers is around 20secs** (w/o scoring the models) and gets you **0.11860 on the Leaderboard** with practically no fuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and globals ü§ñ\n",
    "First we **import all the necessary libraries** and **set a base file path to the data sets**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "BASE_PATH = \"/kaggle/input/house-prices-advanced-regression-techniques/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Superquick intro: What is a Pipeline?\n",
    "A pipeline is a [supercool class that scikit-learn provides](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), which allows to chain so called transformers (that ‚Äì uhmmm.... ‚Äì transform your data) with a final estimator at the end. Let's look a a simple example.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score is: 0.7777\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{BASE_PATH}train.csv\")\n",
    "X = df.select_dtypes(\"number\").drop(\"SalePrice\", axis=1)\n",
    "y = df.SalePrice\n",
    "pipe = make_pipeline(SimpleImputer(), RobustScaler(), LinearRegression())\n",
    "print(f\"The R2 score is: {cross_val_score(pipe, X, y).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"darkred\">How cool is that? \n",
    "> With **only 5 lines of code we imported our training data, separated describing features from the target variable, setup a pipeline with an Imputer (that fills in missing values), a Scaler and a LinearRegression classifier. We crossvalidated and printed out the result.** \n",
    "    \n",
    "It can't be easier than that I think...  \n",
    "\n",
    "Now let's setup a pipeline that is able to **work on our categorical data as well.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score is: 0.8066\n"
     ]
    }
   ],
   "source": [
    "num_cols = df.drop(\"SalePrice\", axis=1).select_dtypes(\"number\").columns\n",
    "cat_cols = df.select_dtypes(\"object\").columns\n",
    "\n",
    "# we instantiate a first Pipeline, that processes our numerical values\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer()),\n",
    "        ('scaler', RobustScaler())])\n",
    "\n",
    "# the same we do for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    \n",
    "# a ColumnTransformer combines the two created pipelines\n",
    "# each tranformer gets the proper features according to ¬´num_cols¬ª and ¬´cat_cols¬ª\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, num_cols),\n",
    "            ('cat', categorical_transformer, cat_cols)])\n",
    "\n",
    "pipe = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', LinearRegression())])\n",
    "\n",
    "X = df.drop(\"SalePrice\", axis=1)\n",
    "y = df.SalePrice\n",
    "print(f\"The R2 score is: {cross_val_score(pipe, X, y).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"darkred\">Even cooler... \n",
    "> With **only 9 lines of code** we processed all our features automagically. Our score improves accordingly.\n",
    "    \n",
    "Now we expand this to a playground for all the regression classifiers you can think of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the estimators ü§ì\n",
    "Since this is meant as a sandbox for experimentation we set a list with 10 common classifiers (and their respective names) which we will use in our pipeline. \n",
    "\n",
    "The initial hyperparameters I have [grid-searched](https://www.kaggle.com/chmaxx/extensive-data-exploration-modelling-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# comment out all classifiers that you don't want to use\n",
    "# and do so for clf_names accordingly\n",
    "classifiers = [\n",
    "               DummyRegressor(),\n",
    "               LinearRegression(n_jobs=-1), \n",
    "               Ridge(alpha=0.003, max_iter=30), \n",
    "               Lasso(alpha=.0005), \n",
    "               ElasticNet(alpha=0.0005, l1_ratio=.9),\n",
    "               KernelRidge(alpha=0.6, kernel=\"polynomial\", degree=2, coef0=2.5),\n",
    "               SGDRegressor(),\n",
    "               SVR(kernel=\"linear\"),\n",
    "               LinearSVR(),\n",
    "               RandomForestRegressor(n_jobs=-1, n_estimators=350, \n",
    "                                     max_depth=12, random_state=1),\n",
    "               GradientBoostingRegressor(n_estimators=500, max_depth=2),\n",
    "               lgb.LGBMRegressor(n_jobs=-1, max_depth=2, n_estimators=1000, \n",
    "                                 learning_rate=0.05),\n",
    "               xgb.XGBRegressor(objective=\"reg:squarederror\", n_jobs=-1, \n",
    "                                max_depth=2, n_estimators=1500, learning_rate=0.075),\n",
    "]\n",
    "\n",
    "clf_names = [\n",
    "            \"dummy\", \n",
    "            \"linear\", \n",
    "            \"ridge\",\n",
    "            \"lasso\",\n",
    "            \"elastic\",\n",
    "            \"kernlrdg\",\n",
    "            \"sgdreg\",\n",
    "            \"svr\",\n",
    "            \"linearsvr\",\n",
    "            \"randomforest\", \n",
    "            \"gbm\", \n",
    "            \"lgbm\", \n",
    "            \"xgboost\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Pipeline üí°\n",
    "We will now setup simple functions to:\n",
    "*     **clean and prepare the data**\n",
    "*     **build the Pipeline**     \n",
    "*     **score models** \n",
    "*     **train models**\n",
    "*     **predict from models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkgreen\">üêö Encapsulate all our feature cleaning and engineering \n",
    "To experiment, just add your code to this function.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data, is_train_data=True):\n",
    "    # add your code for data cleaning and feature engineering here\n",
    "    # e.g. create a new feature from existing ones\n",
    "    data['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\n",
    "\n",
    "    # add here the code that you only want to apply to your training data and not the test set\n",
    "    # e.g. removing outliers from the training data works... \n",
    "    # ...but you cannot remove samples from your test set.\n",
    "    if is_train_data == True:\n",
    "        data = data[data.GrLivArea < 4000]\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkgreen\">üë∑‚Äç‚ôÇÔ∏è Prepare our data for the pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, is_train_data=True):\n",
    "    \n",
    "    # split data into numerical & categorical in order to process seperately in the pipeline \n",
    "    numerical   = df.select_dtypes(\"number\").copy()\n",
    "    categorical = df.select_dtypes(\"object\").copy()\n",
    "    \n",
    "    # for training data only...\n",
    "    # ...convert SalePrice to log values and drop \"Id\" and \"SalePrice\" columns\n",
    "    if is_train_data == True :\n",
    "        SalePrice = numerical.SalePrice\n",
    "        y = np.log1p(SalePrice)\n",
    "        numerical.drop([\"Id\", \"SalePrice\"], axis=1, inplace=True)\n",
    "        \n",
    "    # for the test data: just drop \"Id\" and set \"y\" to None\n",
    "    else:\n",
    "        numerical.drop([\"Id\"], axis=1, inplace=True)\n",
    "        y = None\n",
    "    \n",
    "    # concatenate numerical and categorical data to X (our final training data)\n",
    "    X = pd.concat([numerical, categorical], axis=1)\n",
    "    \n",
    "    # in addition to X and y return the separated columns to use these separetely in our pipeline\n",
    "    return X, y, numerical.columns, categorical.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkgreen\">üë∑‚Äç‚ôÇÔ∏è Create the pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(classifier, num_cols, cat_cols):\n",
    "    # the numeric transformer gets the numerical data acording to num_cols\n",
    "    # the first step is the imputer which imputes all missing values to the mean\n",
    "    # in the second step all numerical data gets scaled by the StandardScaler()\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', make_pipeline(SimpleImputer(strategy='mean'))),\n",
    "        ('scaler', StandardScaler())])\n",
    "    \n",
    "    # the categorical transformer gets all categorical data according to cat_cols\n",
    "    # again: first step is imputing missing values and one hot encoding the categoricals\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    \n",
    "    # the column transformer creates one Pipeline for categorical and numerical data each\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, num_cols),\n",
    "            ('cat', categorical_transformer, cat_cols)])\n",
    "    \n",
    "    # return the whole pipeline with the classifier provided in the function call    \n",
    "    return Pipeline(steps=[('preprocessor', preprocessor), ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:darkgreen\">üå° Score the models with crossvalidation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_models(df):\n",
    "    # retrieve X, y and the seperate columns names\n",
    "    X, y, num_cols, cat_cols = prepare_data(df)\n",
    "    \n",
    "    # since we converted SalePrice to log values, we use neg_mean_squared_error... \n",
    "    # ...rather than *neg_mean_squared_log_error* \n",
    "    scoring_metric = \"neg_mean_squared_error\"\n",
    "    scores = []\n",
    "    \n",
    "    for clf_name, classifier in zip(clf_names, classifiers):\n",
    "        # create a pipeline for each classifier\n",
    "        clf = get_pipeline(classifier, num_cols, cat_cols)\n",
    "        # set a kfold with 3 splits to get more robust scores. \n",
    "        # increase to 5 or 10 to get more precise estimations on models score\n",
    "        kfold = KFold(n_splits=3, shuffle=True, random_state=1)  \n",
    "        # crossvalidate and return the square root of the results\n",
    "        results = np.sqrt(-cross_val_score(clf, X, y, cv=kfold, scoring=scoring_metric))\n",
    "        scores.append([clf_name, results.mean()])\n",
    "\n",
    "    scores = pd.DataFrame(scores, columns=[\"classifier\", \"rmse\"]).sort_values(\"rmse\", ascending=False)\n",
    "    # just for good measure: add the mean of all scores to dataframe\n",
    "    scores.loc[len(scores) + 1, :] = [\"mean_all\", scores.rmse.mean()]\n",
    "    return scores.reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:darkgreen\">  üèãÔ∏è‚Äç‚ôÇÔ∏è Finally: Train the models\n",
    "For each classifier we create and fit a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(df): \n",
    "    X, y, num_cols, cat_cols = prepare_data(df)\n",
    "    pipelines = []\n",
    "    \n",
    "    for clf_name, classifier in zip(clf_names, classifiers):\n",
    "        clf = get_pipeline(classifier, num_cols, cat_cols)\n",
    "        clf.fit(X, y)\n",
    "        pipelines.append(clf)\n",
    "    \n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:darkgreen\">üîÆ Make predictions with trained models  \n",
    "For each fitted pipeline we retrieve predictions for SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_models(df_test, pipelines):\n",
    "    X_test, _ , _, _ = prepare_data(df_test, is_train_data=False)\n",
    "    predictions = []\n",
    "    \n",
    "    for pipeline in pipelines:\n",
    "        preds = pipeline.predict(X_test)\n",
    "        # we return the exponent of the predictions since we have log converted y for training\n",
    "        predictions.append(np.expm1(preds))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ And now: Let's use our pipeline... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{BASE_PATH}train.csv\")\n",
    "df_test = pd.read_csv(f\"{BASE_PATH}test.csv\")\n",
    "\n",
    "# We clean the data\n",
    "df = clean_data(df)\n",
    "df_test = clean_data(df_test, is_train_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>dummy</td>\n",
       "      <td>0.395788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>sgdreg</td>\n",
       "      <td>0.249572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>linearsvr</td>\n",
       "      <td>0.138458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>randomforest</td>\n",
       "      <td>0.138232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.130697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>svr</td>\n",
       "      <td>0.129446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>gbm</td>\n",
       "      <td>0.120207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.120122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>lgbm</td>\n",
       "      <td>0.120102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>ridge</td>\n",
       "      <td>0.116877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>elastic</td>\n",
       "      <td>0.113857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>lasso</td>\n",
       "      <td>0.113624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>kernlrdg</td>\n",
       "      <td>0.113385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>mean_all</td>\n",
       "      <td>0.153874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      classifier      rmse\n",
       "0          dummy  0.395788\n",
       "1         sgdreg  0.249572\n",
       "2      linearsvr  0.138458\n",
       "3   randomforest  0.138232\n",
       "4         linear  0.130697\n",
       "5            svr  0.129446\n",
       "6            gbm  0.120207\n",
       "7        xgboost  0.120122\n",
       "8           lgbm  0.120102\n",
       "9          ridge  0.116877\n",
       "10       elastic  0.113857\n",
       "11         lasso  0.113624\n",
       "12      kernlrdg  0.113385\n",
       "13      mean_all  0.153874"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We score the models on the preprocessed training data\n",
    "my_scores = score_models(df)\n",
    "display(my_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train the models on the whole training set and predict on the test data\n",
    "models = train_models(df)\n",
    "predictions = predict_from_models(df_test, models)\n",
    "# We average over the results of all 12 classifiers (simple ensembling)\n",
    "# we exclude the DummyRegressor and the SGDRegressor: they perform worst...\n",
    "prediction_final = pd.DataFrame(predictions[2:]).mean().T.values\n",
    "\n",
    "submission = pd.DataFrame({'Id': df_test.Id.values, 'SalePrice': prediction_final})\n",
    "submission.to_csv(f\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have feedback? Found errors? Please let me know in the comments. üëåüòé"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
